{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch, torch.utils.data\n",
    "import pickle\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Directory where folders with datasets are stored\n",
    "dir='C:/Users/twest/Documents/school/Master/Deep Learning/Project/Assamese/'\n",
    "\n",
    "# Get the character name for each label\n",
    "GLOBAL_LABEL = 0\n",
    "def get_label(cmap, character_name):\n",
    "  global GLOBAL_LABEL\n",
    "  if character_name not in cmap:\n",
    "    cmap[character_name] = GLOBAL_LABEL\n",
    "    GLOBAL_LABEL += 1\n",
    "  return cmap[character_name]\n",
    "\n",
    "# Create images from the txt files\n",
    "def get_image(filename):\n",
    "  f = open(filename, 'r')\n",
    "  x = None\n",
    "  y = None\n",
    "  start = False\n",
    "  img = Image.new( 'RGB', (4392,4868), \"black\") # create a new black image\n",
    "  draw = ImageDraw.Draw(img)\n",
    "\n",
    "  for line in f:\n",
    "    if 'CHARACTER_NAME:' in line:\n",
    "      character_name = line[len('CHARACTER_NAME:'):].replace(' ', '').rstrip()\n",
    "    if 'PEN_DOWN' in line:\n",
    "      start = True\n",
    "      x = None\n",
    "      y = None\n",
    "    if 'PEN_UP' in line:\n",
    "      start = False\n",
    "      x = None\n",
    "      y = None\n",
    "    if not start:\n",
    "      continue\n",
    "    m = re.match(r'[0-9]+ +[0-9]+ +[0-9]+ +[0-9]+\\.?', line)\n",
    "    if m is not None:\n",
    "      l = str(m.group()).split()\n",
    "      if x is not None:\n",
    "        draw.line((int(l[0]), int(l[1]), x, y), fill=255, width=80) \n",
    "      x = int(l[0])\n",
    "      y = int(l[1])\n",
    "  #img.show()\n",
    "  return character_name, img\n",
    "\n",
    "def change_image(img,img_sz):\n",
    "    img_arr = np.asarray(img) # Convert to array\n",
    "    img_arr = np.sum(img_arr,axis=2) # Get rid of RGB dimension\n",
    "    img_arr = img_arr/np.max(img_arr) # Normalize\n",
    "    img_res = cv2.resize(img_arr, dsize=(img_sz,img_sz), interpolation=cv2.INTER_CUBIC) # Shrink\n",
    "    #img_res = img_res.where(img_res == 0, 1) # Convert to zeros and ones (binarize)\n",
    "    return img_res\n",
    "\n",
    "cmap = {\"BD\": 179, \"NAA\": 93, \"BA\": 111, \"BB\": 61, \"BRR\": 77, \"BL\": 178, \"DRR\": 74, \"JR\": 72, \"PAC\": 88, \"GHA\": 56, \"JJ\": 142, \"DRA\": 125, \"GHN\": 153, \"CCA\": 89, \"JA\": 94, \"CCC\": 141, \"NNN\": 168, \"GHR\": 71, \"KHR\": 69, \"SPH\": 28, \"PA\": 109, \"RA\": 115, \"PRR\": 76, \"KHA\": 34, \"TINI\": 86, \"BHR\": 42, \"GR\": 70, \"XAT\": 91, \"MRR\": 79, \"HRR\": 82, \"BJ\": 144, \"GA\": 45, \"MNA\": 102, \"GN\": 145, \"GM\": 152, \"GL\": 138, \"THB\": 43, \"THA\": 104, \"OI\": 172, \"MND\": 7, \"PHA\": 110, \"PTTT\": 0, \"OU\": 12, \"A\": 0, \"NGG\": 31, \"NGC\": 32, \"HR\": 49, \"NGN\": 35, \"NGH\": 53, \"NGJ\": 37, \"NGK\": 39, \"ST\": 26, \"HN\": 58, \"E\": 106, \"HA\": 122, \"TSR\": 80, \"SKH\": 30, \"PS\": 68, \"KHYA\": 123, \"KR\": 40, \"TTT\": 158, \"SK\": 20, \"TTH\": 55, \"JB\": 143, \"TTB\": 159, \"PTT\": 175, \"PN\": 57, \"TM\": 160, \"PL\": 176, \"EK\": 84, \"EE\": 117, \"SL\": 66, \"DSR\": 81, \"O\": 1, \"SC\": 15, \"DDH\": 48, \"NKH\": 52, \"TRR\": 73, \"TRU\": 41, \"MF\": 60, \"MA\": 113, \"MB\": 5, \"NGKH\": 54, \"ML\": 65, \"MN\": 3, \"CBN\": 131, \"JJB\": 147, \"SSM\": 22, \"SSN\": 24, \"MP\": 2, \"DGH\": 46, \"MXA\": 120, \"FP\": 33, \"FT\": 19, \"ATH\": 92, \"NIYA\": 97, \"WA\": 118, \"FK\": 8, \"FN\": 18, \"REE\": 150, \"GGU\": 50, \"NM\": 171, \"NN\": 156, \"RRG\": 164, \"NA\": 108, \"NB\": 169, \"MDA\": 100, \"UU\": 139, \"MDD\": 154, \"NG\": 67, \"GGN\": 51, \"NT\": 155, \"DD\": 47, \"DUI\": 85, \"XN\": 59, \"NTR\": 4, \"XM\": 36, \"CC\": 140, \"CA\": 78, \"DHRR\": 75, \"NDD\": 165, \"SUNYA\": 83, \"CARI\": 87, \"DHRA\": 126, \"KA\": 23, \"NS\": 170, \"KK\": 132, \"SP\": 27, \"NTT\": 163, \"KM\": 137, \"KL\": 136, \"KS\": 135, \"NTH\": 166, \"SSTH\": 21, \"SN\": 14, \"SM\": 16, \"KT\": 133, \"GDH\": 151, \"SB\": 17, \"NMM\": 157, \"MM\": 181, \"JHA\": 96, \"MNTH\": 38, \"CAY\": 90, \"NDH\": 167, \"DG\": 44, \"ANSR\": 129, \"DB\": 173, \"DA\": 105, \"DV\": 177, \"BXG\": 130, \"LG\": 148, \"LD\": 9, \"LB\": 62, \"LA\": 116, \"LL\": 10, \"LM\": 63, \"TR\": 162, \"LK\": 6, \"TXA\": 119, \"TN\": 146, \"LT\": 13, \"MV\": 182, \"LP\": 11, \"TB\": 180, \"TA\": 103, \"AA\": 95, \"AE\": 161, \"KTT\": 134, \"AYA\": 124, \"STH\": 29, \"DHA\": 107, \"U\": 128, \"AJA\": 114, \"MTA\": 98, \"KTA\": 127, \"QJ\": 174, \"BHM\": 64, \"MDHA\": 101, \"BHA\": 112, \"MTHA\": 99, \"SSB\": 25, \"TT\": 149, \"DXA\": 121}\n",
    "\n",
    "def create_array(type,num_files,output_size):\n",
    "    if type == 'train':\n",
    "        folder_min = 1\n",
    "        folder_max = 36\n",
    "    elif type == 'test':\n",
    "        folder_min = 37\n",
    "        folder_max = 45\n",
    "    else:\n",
    "        print('error')\n",
    "        crash\n",
    "\n",
    "    arr = []\n",
    "    arr2 = []\n",
    "    arr3 = []\n",
    "    arr4 = []\n",
    "    arr5 = []\n",
    "    orig = []\n",
    "    x = []\n",
    "    data = []\n",
    "    labels = []\n",
    "        \n",
    "    for file_nr in range(1,num_files+1):\n",
    "        print(file_nr)\n",
    "        for folder in range(folder_min,folder_max+1):\n",
    "            # Get correct file\n",
    "            file = str(file_nr)+'.'+str(folder)+'.txt'\n",
    "            # Convert\n",
    "            character_name, img = get_image(dir + 'W' + str(folder) + '/' + file)\n",
    "            label = get_label(cmap, character_name)\n",
    "            #img_res = change_image(img, output_size[0])\n",
    "            img_res2 = change_image(img, output_size[1])\n",
    "            img_res3 = change_image(img, output_size[2])\n",
    "            img_res4 = change_image(img, output_size[3])\n",
    "            img_res5 = change_image(img, output_size[4])\n",
    "            # Append\n",
    "            #arr.append(img_res)\n",
    "            arr2.append(img_res2)\n",
    "            arr3.append(img_res3)\n",
    "            arr4.append(img_res4)\n",
    "            arr5.append(img_res5)\n",
    "            #orig.append(img)\n",
    "            x.append(label)\n",
    "    #data = arr\n",
    "    data2 = arr2\n",
    "    data3 = arr3\n",
    "    data4 = arr4\n",
    "    data5 = arr5\n",
    "    labels = x\n",
    "    \n",
    "    if type == 'train':\n",
    "        #np.save('trainingdata'+str(output_size[0])+'_'+str(num_files),data)\n",
    "        np.save('trainingdata'+str(output_size[1])+'_'+str(num_files),data2)\n",
    "        np.save('trainingdata'+str(output_size[2])+'_'+str(num_files),data3)\n",
    "        np.save('trainingdata'+str(output_size[3])+'_'+str(num_files),data4)\n",
    "        np.save('trainingdata'+str(output_size[4])+'_'+str(num_files),data5)\n",
    "        #np.save('alltrainingdata_'+str(num_files),orig)\n",
    "        np.save('traininglabels_'+str(num_files),labels)\n",
    "    elif type == 'test':\n",
    "        #np.save('testdata'+str(output_size[0])+'_'+str(num_files),data)\n",
    "        np.save('testdata'+str(output_size[1])+'_'+str(num_files),data2)\n",
    "        np.save('testdata'+str(output_size[2])+'_'+str(num_files),data3)\n",
    "        np.save('testdata'+str(output_size[3])+'_'+str(num_files),data4)\n",
    "        np.save('testdata'+str(output_size[4])+'_'+str(num_files),data5)\n",
    "        #np.save('alltestdata_'+str(num_files),orig)\n",
    "        np.save('testlabels_'+str(num_files),labels)\n",
    "    else:\n",
    "        print('error')\n",
    "        crash\n",
    "    print('success')\n",
    "        \n",
    "class DataAssamese(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path=None, labels_path=None, \n",
    "                 transform=None, dataset: torch.utils.data.Dataset =None, \n",
    "                 data : np.ndarray = None, labels: np.ndarray = None,\n",
    "                 mean : float = None, std : float = None,\n",
    "                 transform_data=False):\n",
    "        torch.utils.data.Dataset.__init__(self)\n",
    "        \n",
    "        self.data = np.load(data_path)\n",
    "        self.labels = np.load(labels_path)\n",
    "        \n",
    "        if mean == None and std == None:\n",
    "                # stats of the dataset\n",
    "            self.mean = np.mean(self.data[:,:,:])\n",
    "            self.std = np.std(self.data[:,:,:])\n",
    "            # for test set, use mean and std from\n",
    "            # the train set to normalize\n",
    "        else:\n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "            # Normalize by default!\n",
    "        self.normalize = transforms.Normalize(mean=(self.mean,),\n",
    "                                            std = (self.std,))\n",
    "        \n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            self.normalize\n",
    "        ])\n",
    "        \n",
    "        if transform is not None:\n",
    "            self.transforms = transforms.Compose([\n",
    "                  transform\n",
    "              ])\n",
    "        else:\n",
    "            print(\"Using the default transformations\")\n",
    "            self.transforms = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                self.normalize                                \n",
    "            ])\n",
    "            \n",
    "        \n",
    "    \n",
    "    def __getitem__(self,n):\n",
    "        \n",
    "        if self.transform_data:\n",
    "            data = self.data[n]\n",
    "            label = self.labels[n]\n",
    "            # we need to convert the x's to images to apply the transforms\n",
    "            return np.array(self.transforms(data)), label\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            data = self.data[n].astype(np.float32)\n",
    "            \n",
    "            label = self.labels[n].astype(np.float32)\n",
    "            return self.transforms(_x1), self.transforms(_x2), label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "# Create array\n",
    "\n",
    "output_sizes = [384, 192, 96, 48, 24]\n",
    "n_files = 183\n",
    "create_array('train',n_files,output_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "#voor test\n",
    "create_array('test',n_files,output_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'traininglabels96_183.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-67db6105d1cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_labels_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'testlabels96_183.npy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataAssamese\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-f752731a3ba5>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_path, labels_path, transform, dataset, data, labels, mean, std, transform_data)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmean\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mstd\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'traininglabels96_183.npy'"
     ]
    }
   ],
   "source": [
    "train_data_path = 'trainingdata96_183.npy'\n",
    "train_labels_path = 'traininglabels96_183.npy'\n",
    "\n",
    "test_data_path = 'testdata96_183.npy'\n",
    "test_labels_path = 'testlabels96_183.npy'\n",
    "\n",
    "train_d = DataAssamese(train_data_path, train_labels_path)\n",
    "train_loader = torch.utils.data.DataLoader(train_d, batch_size=128, shuffle=True, pin_memory=True, num_workers=4)\n",
    "\n",
    "test_d = DataAssamese(train_data_path, train_labels_path)\n",
    "test_loader = torch.utils.data.DataLoader(train_d, batch_size=128, shuffle=True, pin_memory=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from torch.utils import data\n",
    "\n",
    "def augment_dataset(dat, label):\n",
    "  \"\"\" Augments the dataset and returns a siamese dataset\n",
    "  with 9x as much data, the original data in the argument dataset\n",
    "  plus 8 affine transformations of that input data\"\"\"\n",
    "  # Create a data loader of the dataset\n",
    "  tensors_train = torch.Tensor(dat), torch.Tensor(label).long()\n",
    "  loader = data.DataLoader(TensorDataset(*tensors_train), batch_size=10000)\n",
    "\n",
    "  # Altered samples of the input data\n",
    "  _altered = None\n",
    "  mean = None\n",
    "  std = None\n",
    "\n",
    "  # Check the size of the batches and so on\n",
    "  # Read in batches of 15000, and do it \n",
    "  for j in range(8):\n",
    "      gc.collect()\n",
    "      print(\"starting with round \",j)\n",
    "      for i, (x1, _) in enumerate(loader):\n",
    "          if i % 1 == 0:\n",
    "              print(i)\n",
    "          x1 = np.expand_dims(x1, 1)\n",
    "          # concatenate the arrays by their second axis\n",
    "          _data = x1\n",
    "          _mean = np.mean(_data)\n",
    "          _std = np.std(_data)\n",
    "          if mean is None:\n",
    "            mean = _mean\n",
    "            std = _std\n",
    "          else:\n",
    "            mean = (mean*len(_altered) +  _mean*len(_data))/(len(_altered)+len(_data))\n",
    "            std = (std*len(_altered) +  _std*len(_data))/(len(_altered)+len(_data))\n",
    "          # add them to the dataset\n",
    "          if _altered is None:\n",
    "              _altered = _data\n",
    "          else:\n",
    "              # Concatenate the existing data and the new batch\n",
    "              _altered = np.concatenate((_altered, _data), axis = 0)\n",
    "      \n",
    "      print(f'Size of the datasets -> {_altered.shape}')\n",
    "\n",
    "  # Now create a new dataset with the newly defined data\n",
    "  # Concatenate the original dataset with the new one\n",
    "  d = tensors_train\n",
    "  dat = dat[:,None,:,:]\n",
    "  all_data = np.concatenate((dat, _altered), axis = 0)\n",
    "  labels = np.tile(label, 9)\n",
    "  # Add mean of the original datset\n",
    "  #mean = (mean*len(_altered) +  d.mean*len(d))/(len(_altered)+len(d))\n",
    "  #std = (std*len(_altered) +  d.std*len(d))/(len(_altered)+len(d))\n",
    "  tensors = torch.Tensor(all_data), torch.Tensor(labels).long()\n",
    "  d = DataLoader(TensorDataset(*tensors), batch_size=100)\n",
    "  return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
